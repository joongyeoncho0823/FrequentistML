# -*- coding: utf-8 -*-
"""ECE475_Proj2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C-PvoV24SbbEf9hdeLLBVLAi8Bj5ILwl
"""

# Joongyeon Cho
# ECE475- Frequentist Machine Learning
# Project 2: Logistic Regression

import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import seaborn as sns

# Prepare the data
data = pd.read_csv('./sample_data/heart_disease_data.txt', sep=",", header=None, usecols=range(1, 11))
data.columns = ["sbp","tobacco","ldl","adiposity","famhist","typea","obesity","alcohol","age","chd"]
ones_col = np.ones(len(data))


# data['famhist'] = pd.get_dummies(data['famhist'])['Present']
data["famhist"] = data["famhist"] == "Present"
data["famhist"]=data["famhist"].astype(int)

data

train_data, validate_data, test_data = np.split(data.sample(frac=1, random_state=2), [int(.8*len(data)), int(.9*len(data))])

x_train = train_data.drop(["chd"], axis=1)
y_train = train_data['chd']

x_validate = validate_data.drop(["chd"], axis=1)
y_validate = validate_data['chd']

x_test = test_data.drop(["chd"], axis=1)
y_test = test_data['chd']

x_train = (x_train-x_train.min())/(x_train.max()-x_train.min()) 
x_val = (x_validate-x_validate.min())/(x_validate.max()-x_validate.min()) 
x_test = (x_test-x_test.min())/(x_test.max()-x_test.min()) 

import statsmodels.api as sm
def get_stats():
    results = sm.OLS(y_train, x_train).fit()
    print(results.summary())
    return results
results = get_stats()

results.params

test_data = data['chd'].to_numpy().reshape((len(data),1))

sns.pairplot(
    data, vars=data.columns, kind="scatter", hue='chd',
    palette=['#29CFD0', '#FF0000'], plot_kws=dict(s=30, linewidth=1)
)._legend.remove()

x_train

#------------LOGISTIC REGRESSION WITH STOCHASTIC GRADIENT DESCENT
# set initial thetas
thetas = np.zeros(1, len(x_train.columns))

# use stochastic gradient descent
alpha=0.01 # step size
for i in range(1000):
  h = 1/(1 + np.exp(-np.matmul(thetas, np.transpose(x_train))))
  error = y_train - h
  thetas = thetas + alpha * np.dot(error,x_train)

print("Logistic regression coefficients:",thetas)

# apply thetas 
logistic_sga_predicted = 1/(1 + np.exp(-np.dot(x_test,np.transpose(thetas))))
logistic_sga_predicted_class = [0 if i<=.5 else 1 for i in logistic_sga_predicted]

# XOR lists together to find where classes are different, and divide by length to get % incorrect
pred_incorrect = sum(list(map(bool,logistic_sga_predicted_class)) ^ y_test.astype('bool'))/len(logistic_sga_predicted_class) 
print("Percent of incorrect predictions:",pred_incorrect*100,"%")

std_error = x_train.std() / (len(x_train) ** 0.5)
z_score = np.divide(thetas , std_error)
pd.DataFrame(np.transpose(np.array([thetas, std_error, z_score])), index= x_train.columns, columns= ["Coefficient", "STD error", 'Z-score'])``

#------------LOGISTIC REGRESSION WITH SGD AND REGULARIZATION
# set initial thetas
thetas_reg = np.ones((1000, len(x_train.columns))) # one row for each lambda

# set vector of lambdas
lambdas = np.linspace(0, 0.1, num=10) # I want to test more smaller values than larger values

# initialize scoring vector
vals = np.zeros(lambdas.shape)
# use stochastic gradient descent for each lambda
alpha=0.01 # step size
benchmark = 0.5

for index,lam in enumerate(lambdas):
  for i in range(1000):
    h = 1/(1 + np.exp(-np.dot(x_train,np.transpose(thetas_reg[index]))))
    error = y_train - h

    # update thetas using SGD and L2 regularization penalty
    thetas_reg[index] = thetas_reg[index] + alpha * np.dot(error,x_train) - (lam/(2*len(x_train))) * np.dot(thetas_reg[index], thetas_reg[index])

  log_sgd_reg_pred = 1/(1 + np.exp(-np.dot(x_validate,np.transpose(thetas_reg[index]))))
  log_sgd_reg_pred_class = [0 if i<=benchmark else 1 for i in log_sgd_reg_pred]

  # XOR lists together to find where classes are different, and divide by length to get % incorrect
  pred_incorrect = sum(list(map(bool,log_sgd_reg_pred_class)) ^ y_validate.astype('bool'))/len(log_sgd_reg_pred_class) 
  vals[index] = pred_incorrect

# Find lambda that minimizes error on val set
minpos = np.where(vals == vals.min())
minpos = minpos[0][0] # choose first one if many local minima
best_lam = lambdas[minpos]

print("Best lambda chosen on validation set:",best_lam)
thetas_reg_best = thetas_reg[minpos]
print("Best coefficients chosen on validation set:",thetas_reg_best)

# Apply new thetas to test set and score
log_sgd_reg_pred = 1/(1 + np.exp(-np.dot(x_test,np.transpose(thetas_reg_best))))
log_sgd_reg_pred_class = [0 if i<=benchmark else 1 for i in log_sgd_reg_pred]

# XOR lists together to find where classes are different, and divide by length to get % incorrect
pred_incorrect_sgd = sum(list(map(bool,log_sgd_reg_pred_class)) ^ y_test.astype('bool'))/len(log_sgd_reg_pred_class) 
print("Percent of incorrect predictions:",pred_incorrect_sgd*100,"%")