# -*- coding: utf-8 -*-
"""Frequent_Proj3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XoZo4zRJQ2EKh55TlNgliyCaOXbNtN28
"""

# Re-implement the example in section 7.10.2 using any simple, out of the box classifier (like K nearest neighbors from sci-kit). 
# Reproduce the results for the incorrect and correct way of doing cross-validation.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold
from sklearn.feature_selection import SelectKBest

X = np.zeros((50, 5000))
for i in range(50):
  X[i,:] = np.random.normal(0, 1, 5000)
y = np.ravel(np.random.randint(0, 2, (50,1)))

# Wrong way
scores = []

X_new = SelectKBest(k=100).fit_transform(X,y)
kf = KFold(n_splits=5)
for train_index, test_index in kf.split(X_new):
  x_train, x_test = X_new[train_index], X_new[test_index]
  y_train, y_test = y[train_index], y[test_index]
  
  neigh = KNeighborsClassifier(n_neighbors=1)
  neigh.fit(x_train, y_train)

  y_hat = neigh.predict(x_test)
  score = sum(y_hat == y_test) / len(y_test)
  scores.append(score)

print(f'Cross validation the wrong way: {np.mean(scores) * 100}%')

# Correct Way
scores = []
kf = KFold(n_splits=5)

for train_index, test_index in kf.split(X):
  kbest = SelectKBest(k=50).fit(X[train_index],y[train_index])
  X_new = kbest.transform(X)
  x_train, x_test = X_new[train_index], X_new[test_index]
  y_train, y_test = y[train_index], y[test_index]
  
  neigh = KNeighborsClassifier(n_neighbors=1)
  neigh.fit(x_train, y_train)

  y_hat = neigh.predict(x_test)
  score = sum(y_hat == y_test) / len(y_test)
  scores.append(score)

print(f'Cross validation the correct way: {np.mean(scores) * 100}%')