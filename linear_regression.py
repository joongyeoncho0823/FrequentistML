# -*- coding: utf-8 -*-
"""ECE475_proj1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m4tPexPfiJWYucJthen4rbykDPHVrPsm
"""

# Joongyeon Cho
# ECE475- Frequentist Machine Learning
# Project 1: Linear, Ridge Regression

"""In both the prostate cancer dataset and wine quality dataset, the ridge ression reduced the MSE by a marginal amount, which makes sense because it desensitized the weights to the training data, making it less complex and more applicable to population data. Also, the MSE for the training set was significantly lower than that of the test set, which indicates that the computed weights are modeling the training data much better than the testing set. Lasso regression performed better than the other two regression types on the testing data, but performed worse on the training data. 

In the prostate cancer data, a nonlinear, interaction term was added, (lbph*lcp), which resulted in a slight performance improvement.
Likewise, a nonlinear, interaction term (citric acid * fixed acidity) was added to the wine quality dataset.
"""

import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

data = pd.read_csv('./sample_data/prostate_data.txt', sep="\t", header=None, usecols=range(1, 10))
data.columns = ["lcavol", "lweight", "age", "lbph", "svi", "lcp", "gleason", "pgg45", "lpsa"]
# data.insert(0, "intercept", np.random.rand(len(data)))
data.insert(0, "interaction", np.multiply(data["lbph"], data["lcp"]))
data.describe()

train_data, validate_data, test_data = np.split(data.sample(frac=1, random_state=2), [int(.70*len(data)), int(.85*len(data))])

x_train = train_data.drop(["lpsa"], axis=1)
y_train = train_data['lpsa']

x_validate = validate_data.drop(["lpsa"], axis=1)
y_validate = validate_data['lpsa']

x_test = test_data.drop(["lpsa"], axis=1)
y_test = test_data['lpsa']

x_train = (x_train-x_train.min())/(x_train.max()-x_train.min()) 
x_validate = (x_validate-x_validate.min())/(x_validate.max()-x_validate.min()) 
x_test = (x_test-x_test.min())/(x_test.max()-x_test.min())

"""Linear Regression"""

lr_pinv = np.linalg.inv(x_train.transpose() @ x_train) @ x_train.transpose()
lr_beta = lr_pinv @ y_train


lr_mse_train = mean_squared_error(y_train, np.dot(x_train,lr_beta))
print("MSE for Linear Regression training data:",lr_mse_train)

lr_mse_test = mean_squared_error(y_test, np.dot(x_test,lr_beta))
print("MSE for Linear Regression testing data:",lr_mse_test)

pd.DataFrame(np.tril(x_train.corr()), index=[col for col in x_train.columns], columns=[col for col in x_train.columns])

std_error = x_train.std() / (len(x_train) ** 0.5)
z_score = np.divide(lr_beta.values , std_error)
pd.DataFrame(np.transpose(np.array([lr_beta, std_error, z_score])), index= x_train.columns, columns= ["Coefficient", "STD error", 'Z-score'])

"""Ridge Regression"""

def ridge(lambd):
  rr_pinv = np.linalg.inv(x_train.transpose() @ x_train + np.dot(lambd,np.eye(x_train.shape[1]))) @ x_train.transpose()
  rr_beta = rr_pinv @ y_train
  rss = np.matmul(np.transpose((y_validate - np.matmul(x_validate,rr_beta))),(y_validate - np.matmul(x_validate,rr_beta))) + (lambd*np.dot(np.transpose(rr_beta),rr_beta))
  return rss, rr_beta

# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html
from sklearn import linear_model

def lasso(lambd):
  clf = linear_model.Lasso(alpha=lambd)
  clf.fit(x_train, y_train)
  lr_beta = clf.coef_
  lr_beta[0] = clf.intercept_ #include intercept in betas
  rss_val = np.dot(np.transpose((y_validate - np.dot(x_validate,lr_beta))),(y_validate - np.dot(x_validate,lr_beta))) + (lambd*np.linalg.norm(lr_beta))
  return rss_val,lr_beta


def get_lambd(regression):
  
  lambds = np.linspace(0, 0.5, 1000)
  
  min_rss = 100000
  best_lambd = 0.1
  for lambd in lambds:
    if regression == "ridge":
      rss, beta = ridge(lambd)
    else:
      rss, beta = lasso(lambd)
    if min_rss > rss:
      min_rss = rss
      best_lambd = lambd
  return best_lambd

lambd = get_lambd("ridge")
rss, rr_beta  = ridge(lambd)
rr_mse_train = mean_squared_error(y_train, np.dot(x_train,rr_beta))
rr_mse_test = mean_squared_error(y_test, np.dot(x_test,rr_beta))
print("MSE for Ridge regression training data:", rr_mse_train)
print("MSE for Ridge regression testing data:", rr_mse_test)

"""Ridge regression did not improve on the initial model, lambda = 0, and their MSE is identical since no ridge regression penalty was applied"""

lambds = np.linspace(0,5, 1000)

y_hat = np.zeros((len(lambds),x_train.shape[1]))

for index,val in enumerate(lambds):
    temp = ridge(val) 
    y_hat[index,:] = temp[1] - rr_beta

fig = plt.figure(figsize=(8,8))
for index in range(0,x_train.shape[1]):
  plt.plot(lambds,y_hat[:,index])
plt.axvline(x=lambd, ls="--")
plt.ylabel('Coefficients')
plt.xlabel('$\lambda$')
plt.legend(x_train.columns)
plt.xlim([0,5])
plt.ylim([-0.5,0.5])
plt.show()

"""Lasso Regression
The lasso regression has the lowest MSE for test data out of all three regression types
"""

lambd = get_lambd("lasso")
rss, lr_beta  = lasso(lambd)
lr_mse_train = mean_squared_error(y_train, np.dot(x_train,lr_beta))
lr_mse_test = mean_squared_error(y_test, np.dot(x_test,lr_beta))
print("MSE for Lasso regression training data:", lr_mse_train)
print("MSE for Lasso regression testing data:", lr_mse_test)

lambds = np.linspace(0,2,num=1000)

y_hat = np.zeros((len(lambds),x_train.shape[1]))
s_graph = np.zeros(len(lambds))
best_s = lambd / np.linalg.norm(lasso(lambd)[1])

for index,val in enumerate(lambds):
    y_hat[index,:] = lasso(val)[1]
    s_graph[index] = val/np.linalg.norm(lasso(val)[1])

fig = plt.figure(figsize=(8,8))
for index in range(0,x_train.shape[1]):
  plt.plot(s_graph,y_hat[:,index])
plt.axvline(x=best_s, ls="--")
plt.ylabel('Coefficients')
plt.xlabel('Shrinkage factor s')
plt.legend(x_train.columns)
plt.xlim([0,0.2])
plt.ylim([-0.3,0.7])
plt.show()

"""Red Wine Quality UCI dataset found at: https://archive.ics.uci.edu/ml/datasets/wine+quality"""

data = pd.read_csv('./sample_data/winequality-red.csv', sep=";")
data.insert(0, "interaction", np.multiply(data["fixed acidity"], data["citric acid"]))
data.describe()

train_data, validate_data, test_data = np.split(data.sample(frac=1, random_state=2), [int(.8*len(data)), int(.9*len(data))])

x_train = train_data.drop(["quality"], axis=1)
y_train = train_data['quality']

x_validate = validate_data.drop(["quality"], axis=1)
y_validate = validate_data['quality']

x_test = test_data.drop(["quality"], axis=1)
y_test = test_data['quality']

# Normalize data
x_train = (x_train-x_train.min())/(x_train.max()-x_train.min()) 
x_validate = (x_validate-x_validate.min())/(x_validate.max()-x_validate.min()) 
x_test = (x_test-x_test.min())/(x_test.max()-x_test.min())

lr_pinv = np.linalg.inv(np.dot(x_train.transpose(),x_train)) @ x_train.transpose()
lr_beta = lr_pinv @ y_train


lr_mse_train = mean_squared_error(y_train, np.dot(x_train,lr_beta))
print("MSE for Linear Regression training data:",lr_mse_train)

lr_mse_test = mean_squared_error(y_test, np.dot(x_test,lr_beta))
print("MSE for Linear Regression testing data:",lr_mse_test)

pd.DataFrame(np.tril(x_train.corr()), index=[col for col in x_train.columns], columns=[col for col in x_train.columns])

std_error = x_train.std() / (len(x_train) ** 0.5)
z_score = np.divide(lr_beta.values , std_error)
pd.DataFrame(np.transpose(np.array([lr_beta, std_error, z_score])), index= x_train.columns, columns= ["Coefficient", "STD error", 'Z-score'])

lambd = get_lambd("ridge")
rss, rr_beta  = ridge(lambd)

rr_mse_train = mean_squared_error(y_train, np.dot(x_train,rr_beta))
rr_mse_test = mean_squared_error(y_test, np.dot(x_test,rr_beta))
print("MSE for Ridge regression training data:", rr_mse_train)
print("MSE for Ridge regression testing data:", rr_mse_test)

lambds = np.linspace(0, 5, 1000)

y_hat = np.zeros((len(lambds),x_train.shape[1]))

for index,val in enumerate(lambds):
    temp = ridge(val) 
    y_hat[index,:] = temp[1] - rr_beta

fig = plt.figure()
for index in range(0,x_train.shape[1]):
  plt.plot(lambds,y_hat[:,index])
plt.axvline(x=lambd, ls="--")
plt.ylabel('Coefficients')
plt.xlabel('$\lambda$')
plt.legend(x_train.columns)
plt.xlim([0,5])
plt.ylim([-0.5,0.5])
plt.show()

lambd = get_lambd("lasso")
rss, lr_beta  = lasso(lambd)
lr_mse_train = mean_squared_error(y_train, np.dot(x_train,lr_beta))
lr_mse_test = mean_squared_error(y_test, np.dot(x_test,lr_beta))
print("MSE for Lasso regression training data:", lr_mse_train)
print("MSE for Lasso regression testing data:", lr_mse_test)

lambds = np.linspace(0,2,num=1000)

y_hat = np.zeros((len(lambds),x_train.shape[1]))
s_graph = np.zeros(len(lambds))
best_s = lambd / np.linalg.norm(lasso(lambd)[1])

for index,val in enumerate(lambds):
    y_hat[index,:] = lasso(val)[1]
    s_graph[index] = val/np.linalg.norm(lasso(val)[1])

fig = plt.figure(figsize=(8,8))
for index in range(0,x_train.shape[1]):
  plt.plot(s_graph,y_hat[:,index])
plt.axvline(x=best_s, ls="--")
plt.ylabel('Coefficients')
plt.xlabel('Shrinkage factor s')
plt.legend(x_train.columns)
plt.xlim([0,0.2])
plt.ylim([-0.2,0.7])
plt.show()