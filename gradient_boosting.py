# -*- coding: utf-8 -*-
"""FrequentistML_Proj4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yZBKZsrJJj-rDvm-sd71D4o4UvAahhVo
"""

import pandas as pd
import numpy as np

from sklearn.datasets import fetch_california_housing

california = fetch_california_housing()

df = pd.DataFrame(california.data,
                             columns=california.feature_names)
df['MedHouseValue'] = pd.Series(california.target)
df.head()

features = ['Population', 'AveBedrms', 'AveRooms', 'HouseAge',
            'Latitude', 'AveOccup', 'Longitude', 'MedInc']
X, y = df[features].values, df['MedHouseValue'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Baseline
from sklearn.metrics import mean_absolute_error


y_train_mean = np.mean(y_train)
y_test_hat = np.full(shape=y_test.shape, fill_value=y_train_mean)
const_mae = mean_absolute_error(y_test, y_test_hat)
print(f'MAE of optimal constant predictor {const_mae:.2f}')

#!pip install xgboost==1.6.0
import xgboost as xgb
from sklearn.metrics import mean_absolute_error

xg_reg = xgb.XGBRegressor(objective ="reg:pseudohubererror", eval_metric= mean_absolute_error, learning_rate = 0.1,max_leaves = 6,
                          booster= "dart",
                          n_estimators = 800)
history = xg_reg.fit(
    X_train, 
    y_train,
    early_stopping_rounds = 10,
    verbose = 1, 
    eval_set=[(X_train, y_train), (X_test, y_test)]
)

"""Unlike in "Elements of Statistical Learning" 10.14.1, we only ran it for only half the number of iterations because of early stopping while the textbook specified they ran 800 iterations. However, the results were similar."""

from sklearn.metrics import r2_score

y_test_hat = xg_reg.predict(X_test)
reg_mae = mean_absolute_error(y_test, y_test_hat)
reg_r2 = r2_score(y_test, y_test_hat)
print(f'XGBoost MAE={reg_mae:.2f}, R^2={reg_r2:.2f}')

from matplotlib import pyplot as plt

# Figure 10.13 
train_error = xg_reg.evals_result()['validation_0']['mean_absolute_error']
test_error = xg_reg.evals_result()['validation_1']['mean_absolute_error']
fig, ax = plt.subplots(figsize=(5.9, 3.39), dpi=110)
ax.set_xlabel('Iterations M', fontsize=8)
ax.set_ylabel('Absolute Error', fontsize=8)
ax.plot(range(len(train_error)), train_error,
        label='Train Error', color='#10A47B', linewidth=0.7)
ax.plot(range(len(test_error)), test_error,
        label='Test Error', color='#CC79A7', linewidth=0.7)
ax.text(ax.get_xlim()[0], 0.95, 'Training and Test Absolute Error',
        fontsize=9)
_ = ax.legend(loc='upper right', prop={'size': 8})

"""This resulting figure differs from Figure 10.13 in the textbook. While the train error is similar to the textbook, it shows that the train error is much lower than in the textbook. This suggests our model is overfitting our data. In conjunction with Figure 10.14 below, the graphs suggest that our model is overfitting the median income parameter. """

relative_importance = list(xg_reg.get_booster().get_score(importance_type='gain').values())
max_importance = max(relative_importance)
relative_importance = np.array(relative_importance)
relative_importance = np.divide(100*relative_importance, max_importance)
relative_importance

# Figure 10.14
yticks = np.arange(len(relative_importance))
yticklabels = features

fig, ax = plt.subplots(figsize=(4.3, 3), dpi=150)
bars = ax.barh(yticks, relative_importance, height=0.8, color='red')
plt.setp(ax, yticks=yticks, yticklabels= yticklabels)

"""This resulting bar graph is slightly different from the textbook. Median Income is still the most important indicator, and Average Occupation, Latitutude & Longitude are the next most important indicators. However, unlike the textbook, population is a stronger indicator than average # of bedrooms and average # of rooms. """

# Figure 10.15
from sklearn.inspection import (partial_dependence, 
                                PartialDependenceDisplay)
features = ['Population', 'AveBedrms', 'AveRooms', 'HouseAge',
            'Latitude', 'AveOccup', 'Longitude', 'MedInc']
fig, ax = plt.subplots(4, 2, figsize=(10, 12))
PartialDependenceDisplay.from_estimator(xg_reg, X_train, [0,1,2,3,4,5,6,7], feature_names=features, ax=ax, n_cols=2)
fig.suptitle('Partial Dependence Plots')
fig.tight_layout();

"""Figure 10.15 in the textbook shows the partial dependence of four variables. Here, we show the partial dependence for all 8 variables. Median Income show a positive dependence with the target variable, which makes sense since higher income would generally indicate to higher home value."""

# Dataset from: https://archive.ics.uci.edu/ml/datasets/Wine+Quality
df = pd.read_csv("winequality-red (2).csv", sep=';', header='infer')
df.describe()

X,y = df.drop(["quality"], axis = 1), df["quality"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

import xgboost as xgb
from sklearn.metrics import mean_absolute_error

xg_reg = xgb.XGBRegressor(objective ="reg:pseudohubererror", eval_metric= mean_absolute_error, learning_rate = 0.01,
                          booster= "dart",
                          n_estimators = 800)
history = xg_reg.fit(
    X_train, 
    y_train,
    early_stopping_rounds = 10,
    verbose = 1, 
    eval_set=[(X_train, y_train), (X_test, y_test)]
)

relative_importance = list(xg_reg.get_booster().get_score(importance_type='gain').values())
max_importance = max(relative_importance)
relative_importance = np.array(relative_importance)
relative_importance = np.divide(100*relative_importance, max_importance)
relative_importance

# Figure 10.14
yticks = np.arange(len(relative_importance))
yticklabels = X.columns

fig, ax = plt.subplots(figsize=(4.3, 3), dpi=150)
bars = ax.barh(yticks, relative_importance, height=0.8, color='red')
plt.setp(ax, yticks=yticks, yticklabels= yticklabels)

# Figure 10.15
from sklearn.inspection import (partial_dependence, 
                                PartialDependenceDisplay)
features = ['Population', 'AveBedrms', 'AveRooms', 'HouseAge',
            'Latitude', 'AveOccup', 'Longitude', 'MedInc']
fig, ax = plt.subplots(2, 2, figsize=(10, 12))
PartialDependenceDisplay.from_estimator(xg_reg, X_train, [0,1,2,3], feature_names=["alcohol", "sulphates","pH","density"], ax=ax, n_cols=2)
fig.suptitle('Partial Dependence Plots')
fig.tight_layout();