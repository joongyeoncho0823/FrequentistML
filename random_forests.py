# -*- coding: utf-8 -*-
"""random_forests.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dhynf8WYDij3QVTcc0zlRdA2AxG7ydkB
"""

import pandas as pd
import numpy as np

from sklearn.datasets import fetch_california_housing

california = fetch_california_housing()

df = pd.DataFrame(california.data,
                             columns=california.feature_names)
df['MedHouseValue'] = pd.Series(california.target)
df.head()

features = ['Population', 'AveBedrms', 'AveRooms', 'HouseAge',
            'Latitude', 'AveOccup', 'Longitude', 'MedInc']
X, y = df[features].values, df['MedHouseValue'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

from sklearn.ensemble import RandomForestRegressor as rfr
from sklearn.ensemble import GradientBoostingRegressor as gbr
from sklearn.metrics import mean_absolute_error

"""FIGURE 15.3. Random forests compared to gradient boosting on the California
housing data. The curves represent mean absolute error on the test data as a
function of the number of trees in the models. Two random forests are shown, with
m = 2 and m = 6. The two gradient boosted models use a shrinkage parameter
Î½ = 0.05 in (10.41), and have interaction depths of 4 and 6. The boosted models
outperform random forests.
"""

# Params

trees = {
    'rf_2': rfr(max_features = 2),
    'rf_6': rfr(max_features = 6),
    'gbr_depth4': gbr(max_depth = 4),
    'gbr_depth6': gbr(max_depth = 6)
}

histories = {}
for name, tree in trees.items():
  histories[name] = []
  for i in range (1, 150):
    tree.set_params(n_estimators = i)
    tree.fit(X_train, y_train)

    y_hat = tree.predict(X_test)

    histories[name].append(mean_absolute_error(y_test, y_hat))

import matplotlib.pyplot as plt


num_trees = range(1, 150)
for name, mae in histories.items():
  plt.plot(num_trees, mae, label = name, marker='o')
plt.xlabel('Number of Trees')
plt.ylabel('Test Average Absolute Error')
plt.title('California Housing Data')
plt.legend()
plt.show()

f, ax = plt.subplots(2,2, dpi=150, figsize=(10,10))
i = 0
for name, tree in trees.items():
  feature_importances_ = tree.feature_importances_
  yticks = np.arange(len(feature_importances_))
  ax[i//2, i%2].barh(yticks, feature_importances_)
  ax[i//2, i%2].set_title(name)
  plt.setp(ax, yticks=yticks, yticklabels= df.columns)
  i = i+1

plt.show()

# Different Dataset
df = pd.read_csv("winequality-red (1).csv", sep=';', header='infer')
df.describe()

X,y = df.drop(["quality"], axis = 1), df["quality"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

trees = {
    'rf_2': rfr(max_features = 2),
    'rf_6': rfr(max_features = 6),
    'gbr_depth4': gbr(max_depth = 4),
    'gbr_depth6': gbr(max_depth = 6)
}

histories = {}
for name, tree in trees.items():
  histories[name] = []
  for i in range (1, 150):
    tree.set_params(n_estimators = i)
    tree.fit(X_train, y_train)

    y_hat = tree.predict(X_test)

    histories[name].append(mean_absolute_error(y_test, y_hat))

num_trees = range(1, 150)
for name, mae in histories.items():
  plt.plot(num_trees, mae, label = name, marker='o')
plt.xlabel('Number of Trees')
plt.ylabel('Test Average Absolute Error')
plt.title('Wine Quality Data')
plt.legend()
plt.show()

f, ax = plt.subplots(2,2, dpi=150, figsize=(10,10))
i = 0
for name, tree in trees.items():
  feature_importances_ = tree.feature_importances_
  yticks = np.arange(len(feature_importances_))
  ax[i//2, i%2].barh(yticks, feature_importances_)
  ax[i//2, i%2].set_title(name)
  plt.setp(ax, yticks=yticks, yticklabels= df.columns)
  i = i+1

plt.show()